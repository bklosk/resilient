#  Photogrammetree ğŸŒ³ 
## Climate Disaster Mitigation ğŸŒ

This project uses **LiDAR** data ğŸ›°ï¸ and **orthophotos** ğŸ“¸ combined with **3D semantic segmentation** ğŸ¤– and FEA/FDS to identify and analyze mutable candidates for climate risk mitigation.

## The Goal ğŸ¯

Our aim is to pinpoint specific areas or objects within a 3D environment (e.g., vegetation, buildings, infrastructure) that can be modified or managed to reduce climate-related risks like flooding, heat islands, or wildfire spread.

For example, we could:
*   Identify vegetation likely to be a vector for fire spread to buildings.
*   Find optimal placements for floodwalls, sandbags, or home risers.
*   Detect rooftops suitable for conversion to steel roofs ğŸŒ¿ or solar panel installations â˜€ï¸.
*   Assess infrastructure vulnerabilities that need reinforcement.

## Technologies ğŸ› ï¸

*   **LiDAR:** For creating accurate 3D point clouds of the environment.
*   **Orthophotos:** Georeferenced aerial imagery providing color and texture information.
*   **3D Semantic Segmentation:** Machine learning models to classify objects within the 3D point cloud (e.g., ground, buildings, low vegetation, high vegetation).

## Workflow Overview (Conceptual) ğŸ—ºï¸

```
[LiDAR Data ğŸ›°ï¸ + Orthophotos ğŸ“¸] --> [Preprocessing & Colorization ğŸ¨] --> [3D Semantic Segmentation ğŸ¤–] --> [Candidate Identification ğŸ¯] --> [Physics Simulation & ML Analysis ğŸŒªï¸ğŸ”¥ğŸŒŠ] --> [Mitigation Strategies & Visualization ğŸ“Š]
```

1.  **Data Acquisition & Preprocessing:**
    *   Collect LiDAR scans ğŸ“¡ and orthophotos ğŸ–¼ï¸.
    *   Align and fuse these datasets.
    *   Colorize the LiDAR point cloud using orthophoto imagery (Implemented in `scripts/pipeline.py`!).
2.  **3D Semantic Segmentation:**
    *   Train or use a pre-trained model to classify points in the cloud (e.g., using Open3D-ML) ğŸ§ .
3.  **Candidate Identification & Analysis:**
    *   Develop algorithms to analyze the segmented point cloud.
    *   Identify "mutable" objects/areas based on their class and properties.
    *   Evaluate their potential for climate risk mitigation.
4.  **Physics-Based Simulation & ML Analysis ğŸŒªï¸ğŸ”¥ğŸŒŠ:**
    *   Run Finite Element Analysis (FEA) and Fire Dynamics Simulator (FDS) simulations on the segmented point cloud.
    *   Simulate physical phenomena like fires, floods, and high winds.
    *   Utilize Machine Learning to analyze simulation results and refine mitigation strategies.
5.  **Visualization & Reporting:**
    *   Visualize the results in 3D.
    *   Generate reports for decision-making.

---

## Project Setup and Execution

### Prerequisites

- Python 3.10 or higher
- Git (for cloning the repository, if applicable)

### Setup in a GitHub Codespace or Local Environment

1.  **Clone the repository (if you haven't already):**
    ```bash
    git clone <repository-url>
    cd photogrammetry
    ```

2.  **Create and activate a Python virtual environment:**
    It's highly recommended to use a virtual environment to manage project dependencies.
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
    (On Windows, use `venv\Scripts\activate`)

3.  **Install Python dependencies:**
    The required Python packages are listed in `requirements.txt`.
    ```bash
    pip install -r requirements.txt
    ```

### Running the Pipeline ğŸš€

The main pipeline script is `scripts/pipeline.py`. This script currently focuses on colorizing a hardcoded point cloud using satellite imagery and saves the processed data.

To run the pipeline:

1.  **Ensure your virtual environment is activated.** âœ…
    ```bash
    source venv/bin/activate
    ```
2.  **Navigate to the `scripts` directory (optional, but paths in the script are relative to its location):**
    ```bash
    cd scripts
    ```
3.  **Execute the pipeline script:**
    ```bash
    python pipeline.py
    ```
    If you are in the root directory of the project, you can run it as:
    ```bash
    python scripts/pipeline.py
    ```

The script will process the data and print status messages to the console. Upon completion, it will save the colorized point cloud to a predefined location (e.g., `data/colorized_point_cloud.laz`). No separate visualization window will be opened by this script.

### Running Tests

After installing the dependencies you can run the API test suite with:

```bash
pytest
```

The tests use lightweight stubs so they finish quickly without needing external data downloads.

---

Fighting climate change, one point cloud at a time! âœ¨ğŸŒğŸŒ³
